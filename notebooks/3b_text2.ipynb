{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text analysis II\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "- Preprocess text.\n",
    "- Extract features.\n",
    "    - Word counts\n",
    "    - Term frequenct\n",
    "    - Word and document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jkiley/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob, Word\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text\n",
    "\n",
    "There are a number of common ways to preprocess text for use in machine learning and other text analysis models.\n",
    "While these things are often helpful, feel free to experiment with your own models and text corpora.\n",
    "\n",
    "\n",
    "Note that we will look at some of this functionality in TextBlob, though as we will see, we often use scikit-learn's tools for these tasks.\n",
    "However, textblob makes it easy to see how these tools work.\n",
    "\n",
    "- lower case\n",
    "- punctuation removed\n",
    "- POS tagging\n",
    "- lemmatization\n",
    "- n-grams\n",
    "- stop words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ultimately, we want to turn our text into a matrix that gives the algorithm information to categorize text. That is more difficult if we miss the same words due to case, punctuation, or common words that don't help predict. So, we can clean our text to potentially make our predictions better.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text_1 = ('Ultimately, we want to turn our text into a matrix that '\n",
    "                 'gives the algorithm information to categorize text. That '\n",
    "                 'is more difficult if we miss the same words due to case, '\n",
    "                 'punctuation, or common words that don\\'t help predict. '\n",
    "                 'So, we can clean our text to potentially make our '\n",
    "                 'predictions better.')\n",
    "example_text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'ultimately': 1,\n",
       "             'we': 3,\n",
       "             'want': 1,\n",
       "             'to': 4,\n",
       "             'turn': 1,\n",
       "             'our': 3,\n",
       "             'text': 3,\n",
       "             'into': 1,\n",
       "             'a': 1,\n",
       "             'matrix': 1,\n",
       "             'that': 3,\n",
       "             'gives': 1,\n",
       "             'the': 2,\n",
       "             'algorithm': 1,\n",
       "             'information': 1,\n",
       "             'categorize': 1,\n",
       "             'is': 1,\n",
       "             'more': 1,\n",
       "             'difficult': 1,\n",
       "             'if': 1,\n",
       "             'miss': 1,\n",
       "             'same': 1,\n",
       "             'words': 2,\n",
       "             'due': 1,\n",
       "             'case': 1,\n",
       "             'punctuation': 1,\n",
       "             'or': 1,\n",
       "             'common': 1,\n",
       "             'do': 1,\n",
       "             \"n't\": 1,\n",
       "             'help': 1,\n",
       "             'predict': 1,\n",
       "             'so': 1,\n",
       "             'can': 1,\n",
       "             'clean': 1,\n",
       "             'potentially': 1,\n",
       "             'make': 1,\n",
       "             'predictions': 1,\n",
       "             'better': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_blob_1 = TextBlob(example_text_1)\n",
    "e_blob_1.word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice a few things about the dictionary above.\n",
    "\n",
    "1. This text has been **tokenized**, meaning that it has been split into tokens that have meaning (words in this case).\n",
    "1. textblob make the words lowercase before counting them. The word \"that\" appears in the original text both capitalized and lower case. This is perhaps the most common transformation of all, so it is not surprising that it does that for us automatically.\n",
    "1. The punctuation has been removed. That's not always something we will want, but it is quite helpful in most cases.\n",
    "1. The word \"don't\" was split into `'do'` and ``\"n't\"``. The tokenizer is smart enough to separate it so that the negation is captured separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many times, we would like to consider parts of speech, and there are quite good models for finding this information for words.\n",
    "textblob has this functionality built in.\n",
    "For some tasks, it can be helpful to treat words used as different parts of speech as different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ultimately', 'RB'),\n",
       " ('we', 'PRP'),\n",
       " ('want', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('turn', 'VB'),\n",
       " ('our', 'PRP$'),\n",
       " ('text', 'NN'),\n",
       " ('into', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('matrix', 'NN')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use slicing to look at the first ten.\n",
    "e_blob_1.tags[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we may want to reduce words to their base or **lemmatized** form in order to construct better counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learning'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word('learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learn'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We tell the lemmatize method the part of speech.\n",
    "Word('learning').lemmatize('v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common transformation is using more than one word at a time to capture context.\n",
    "These multi-word groups are called **n-grams**.\n",
    "We do have to be careful here, as the dimensionality (and, thus, computational intensity) grows very quickly.\n",
    "\n",
    "**Note:** we would typically add the n-grams to the single words as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of words alone:  39\n",
      "Length of n-grams of 2: 51\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of words alone:  {len(e_blob_1.word_counts)}')\n",
    "print(f'Length of n-grams of 2: {len(e_blob_1.ngrams(2))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['ultimately', 'we', 'want', 'to', 'turn', 'our', 'text', 'into', 'a', 'matrix', 'that', 'gives', 'the', 'algorithm', 'information', 'to', 'categorize', 'text', 'that', 'is', 'more', 'difficult', 'if', 'we', 'miss', 'the', 'same', 'words', 'due', 'to', 'case', 'punctuation', 'or', 'common', 'words', 'that', 'do', \"n't\", 'help', 'predict', 'so', 'we', 'can', 'clean', 'our', 'text', 'to', 'potentially', 'make', 'our', 'predictions', 'better'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_blob_1.words.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ultimately',\n",
       " 'want',\n",
       " 'turn',\n",
       " 'text',\n",
       " 'matrix',\n",
       " 'gives',\n",
       " 'algorithm',\n",
       " 'information',\n",
       " 'categorize',\n",
       " 'text',\n",
       " 'difficult',\n",
       " 'miss',\n",
       " 'words',\n",
       " 'due',\n",
       " 'case',\n",
       " 'punctuation',\n",
       " 'common',\n",
       " 'words',\n",
       " \"n't\",\n",
       " 'help',\n",
       " 'predict',\n",
       " 'clean',\n",
       " 'text',\n",
       " 'potentially',\n",
       " 'make',\n",
       " 'predictions',\n",
       " 'better']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_blob_1_stop = [w for w in e_blob_1.words.lower() \n",
    "                 if w not in stopwords.words('english')]\n",
    "e_blob_1_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word count features\n",
    "\n",
    "We're going to use these sentences as an example to see how the transformation work, though the patterns we will see are generally quite common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice description that comes with this dataset.\n",
    "# You can uncomment and run it yourself if you like.\n",
    "# print(news_test['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn`'s text utilities do a lot of feature extraction for us relatively easily.\n",
    "We will look at them in a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the defaults.\n",
    "test_cv = CountVectorizer()\n",
    "test_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note a few things:\n",
    "\n",
    "1. By default, `lowercase=True`. As we discussed before, this is a transform that is nearly universal.\n",
    "1. It has a default of `ngram_range=(1, 1)`, but we can see that we can specify n-grams.\n",
    "1. It can filter stop words, but it is off be default. As the [documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#stop-words) notes, there are reasons to worry about stop words.\n",
    "1. If we want to override the built-in behavior, it allows us to pass in our own functions for the `preprocessor` and `tokenizer` arguments.\n",
    "1. Note that we do not have POS tagging built-in, but we could preprocess the text ourselves to feed in data with tags.\n",
    "\n",
    "Let's see some output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['allows', 'and', 'arguments', 'behavior', 'built', 'but', 'could', 'data', 'do', 'feed', 'for', 'functions', 'have', 'if', 'in', 'it', 'not', 'note', 'our', 'ourselves', 'override', 'own', 'pass', 'pos', 'preprocess', 'preprocessor', 'tagging', 'tags', 'text', 'that', 'the', 'to', 'tokenizer', 'us', 'want', 'we', 'with']\n",
      "[[1 1 1 1 1 0 0 0 0 0 1 1 0 1 2 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 2 2 1 1 1 1\n",
      "  0]\n",
      " [0 0 0 0 1 1 1 1 1 1 0 0 1 0 2 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 0 2\n",
      "  1]]\n"
     ]
    }
   ],
   "source": [
    "test_sentences = ['If we want to override the built-in behavior, '\n",
    "                  'it allows us to pass in our own functions for the '\n",
    "                  ' preprocessor and tokenizer arguments.',\n",
    "                  'Note that we do not have POS tagging built-in, '\n",
    "                  'but we could preprocess the '\n",
    "                  'text ourselves to feed in data with tags.']\n",
    "test_sent_vec = test_cv.fit_transform(test_sentences)\n",
    "print(test_cv.get_feature_names())\n",
    "print(test_sent_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['allows', 'allows us', 'and', 'and tokenizer', 'arguments', 'behavior', 'behavior it', 'built', 'built in', 'but', 'but we', 'could', 'could preprocess', 'data', 'data with', 'do', 'do not', 'feed', 'feed in', 'for', 'for the', 'functions', 'functions for', 'have', 'have pos', 'if', 'if we', 'in', 'in behavior', 'in but', 'in data', 'in our', 'it', 'it allows', 'not', 'not have', 'note', 'note that', 'our', 'our own', 'ourselves', 'ourselves to', 'override', 'override the', 'own', 'own functions', 'pass', 'pass in', 'pos', 'pos tagging', 'preprocess', 'preprocess the', 'preprocessor', 'preprocessor and', 'tagging', 'tagging built', 'tags', 'text', 'text ourselves', 'that', 'that we', 'the', 'the built', 'the preprocessor', 'the text', 'to', 'to feed', 'to override', 'to pass', 'tokenizer', 'tokenizer arguments', 'us', 'us to', 'want', 'want to', 'we', 'we could', 'we do', 'we want', 'with', 'with tags']\n",
      "[[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 2 1 0 0 1 1 1 0 0\n",
      "  0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 2 1 1 0 2 0 1 1 1 1 1\n",
      "  1 1 1 1 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 2 0 1 1 0 0 0 1 1\n",
      "  1 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0\n",
      "  0 0 0 2 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Let's see what happens with n-grams of 2.\n",
    "test_cv_2 = CountVectorizer(ngram_range=(1, 2))\n",
    "test_sent_vec_2 = test_cv_2.fit_transform(test_sentences)\n",
    "print(test_cv_2.get_feature_names())\n",
    "print(test_sent_vec_2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue that may seem obvious from our discussion of stop words earlier is that some words don't do a lot for us in terms of prediction.\n",
    "Another strategy for dealing with that issue is weighting terms such that those that are less frequent receive a higher weight and vice versa.\n",
    "We call this **term frequency times inverse document frequency** or tf-idf.\n",
    "\n",
    "Another issue you may have thought of is that we're using raw counts above.\n",
    "Longer documents will naturally have higher counts, so we can normalize those values if we choose (like the example below).\n",
    "It is not that important for our examples, but some models are sensitive to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again, let's look at it.\n",
    "test_tt_1 = TfidfTransformer()\n",
    "test_tt_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, by default, it both normalizes and uses idf, but we can change those arguments if we choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14809752 0.14809752 0.14809752 0.14809752 0.14809752 0.14809752\n",
      "  0.14809752 0.1053726  0.1053726  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.14809752 0.14809752 0.14809752 0.14809752 0.\n",
      "  0.         0.14809752 0.14809752 0.2107452  0.14809752 0.\n",
      "  0.         0.14809752 0.14809752 0.14809752 0.         0.\n",
      "  0.         0.         0.14809752 0.14809752 0.         0.\n",
      "  0.14809752 0.14809752 0.14809752 0.14809752 0.14809752 0.14809752\n",
      "  0.         0.         0.         0.         0.14809752 0.14809752\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2107452  0.14809752 0.14809752 0.         0.2107452\n",
      "  0.         0.14809752 0.14809752 0.14809752 0.14809752 0.14809752\n",
      "  0.14809752 0.14809752 0.14809752 0.1053726  0.         0.\n",
      "  0.14809752 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.10840958 0.10840958 0.15236588 0.15236588 0.15236588\n",
      "  0.15236588 0.15236588 0.15236588 0.15236588 0.15236588 0.15236588\n",
      "  0.15236588 0.         0.         0.         0.         0.15236588\n",
      "  0.15236588 0.         0.         0.21681916 0.         0.15236588\n",
      "  0.15236588 0.         0.         0.         0.15236588 0.15236588\n",
      "  0.15236588 0.15236588 0.         0.         0.15236588 0.15236588\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.15236588 0.15236588 0.15236588 0.15236588 0.         0.\n",
      "  0.15236588 0.15236588 0.15236588 0.15236588 0.15236588 0.15236588\n",
      "  0.15236588 0.10840958 0.         0.         0.15236588 0.10840958\n",
      "  0.15236588 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.21681916 0.15236588 0.15236588\n",
      "  0.         0.15236588 0.15236588]]\n"
     ]
    }
   ],
   "source": [
    "test_sent_tfidf_1 = test_tt_1.fit_transform(test_sent_vec_2.toarray())\n",
    "print(test_sent_tfidf_1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we looked at a number of the intermediate states, these tasks are common enough that the `TfidfVectorizer` class bundles together both `CountVectorizer` and `TfidfTransformer` into one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that we get the same result in one step.\n",
    "test_tv_1 = TfidfVectorizer(ngram_range=(1, 2))\n",
    "test_sent_tv_1 = test_tv_1.fit_transform(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14809752 0.14809752 0.14809752 0.14809752 0.14809752 0.14809752\n",
      "  0.14809752 0.1053726  0.1053726  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.14809752 0.14809752 0.14809752 0.14809752 0.\n",
      "  0.         0.14809752 0.14809752 0.2107452  0.14809752 0.\n",
      "  0.         0.14809752 0.14809752 0.14809752 0.         0.\n",
      "  0.         0.         0.14809752 0.14809752 0.         0.\n",
      "  0.14809752 0.14809752 0.14809752 0.14809752 0.14809752 0.14809752\n",
      "  0.         0.         0.         0.         0.14809752 0.14809752\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2107452  0.14809752 0.14809752 0.         0.2107452\n",
      "  0.         0.14809752 0.14809752 0.14809752 0.14809752 0.14809752\n",
      "  0.14809752 0.14809752 0.14809752 0.1053726  0.         0.\n",
      "  0.14809752 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.10840958 0.10840958 0.15236588 0.15236588 0.15236588\n",
      "  0.15236588 0.15236588 0.15236588 0.15236588 0.15236588 0.15236588\n",
      "  0.15236588 0.         0.         0.         0.         0.15236588\n",
      "  0.15236588 0.         0.         0.21681916 0.         0.15236588\n",
      "  0.15236588 0.         0.         0.         0.15236588 0.15236588\n",
      "  0.15236588 0.15236588 0.         0.         0.15236588 0.15236588\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.15236588 0.15236588 0.15236588 0.15236588 0.         0.\n",
      "  0.15236588 0.15236588 0.15236588 0.15236588 0.15236588 0.15236588\n",
      "  0.15236588 0.10840958 0.         0.         0.15236588 0.10840958\n",
      "  0.15236588 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.21681916 0.15236588 0.15236588\n",
      "  0.         0.15236588 0.15236588]]\n"
     ]
    }
   ],
   "source": [
    "print(test_sent_tv_1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vector features\n",
    "\n",
    "As we discussed earlier, word vectors represent words as vectors in a vector space.\n",
    "The embeddings below are from Stanford's [GloVe](https://github.com/stanfordnlp/GloVe) project, specifically the 100-dimensional version of the Wikipedia 2014 + Gigaword 5 data.\n",
    "\n",
    "Here, I have extracted a small subset of the embeddings for the words in the `e_blob_1_stop` list above.\n",
    "The full data is quite large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ultimately</th>\n",
       "      <td>0.45483</td>\n",
       "      <td>-0.102290</td>\n",
       "      <td>0.35265</td>\n",
       "      <td>0.16507</td>\n",
       "      <td>0.333910</td>\n",
       "      <td>-0.033643</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>-0.093160</td>\n",
       "      <td>0.60292</td>\n",
       "      <td>-0.26628</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.352140</td>\n",
       "      <td>-0.47388</td>\n",
       "      <td>-0.192560</td>\n",
       "      <td>0.27630</td>\n",
       "      <td>-0.053913</td>\n",
       "      <td>0.34036</td>\n",
       "      <td>0.237230</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.10977</td>\n",
       "      <td>0.18678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>-0.17124</td>\n",
       "      <td>0.564470</td>\n",
       "      <td>0.34667</td>\n",
       "      <td>-0.56711</td>\n",
       "      <td>-0.656750</td>\n",
       "      <td>0.120810</td>\n",
       "      <td>-0.768630</td>\n",
       "      <td>0.072832</td>\n",
       "      <td>0.42237</td>\n",
       "      <td>-0.10464</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013218</td>\n",
       "      <td>-0.20853</td>\n",
       "      <td>0.052186</td>\n",
       "      <td>-0.86911</td>\n",
       "      <td>-0.858160</td>\n",
       "      <td>-0.23443</td>\n",
       "      <td>0.057799</td>\n",
       "      <td>0.031150</td>\n",
       "      <td>0.48789</td>\n",
       "      <td>0.69311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>turn</th>\n",
       "      <td>-0.08525</td>\n",
       "      <td>0.021085</td>\n",
       "      <td>0.30965</td>\n",
       "      <td>-0.17603</td>\n",
       "      <td>0.088869</td>\n",
       "      <td>0.285050</td>\n",
       "      <td>-0.344560</td>\n",
       "      <td>0.113960</td>\n",
       "      <td>0.29212</td>\n",
       "      <td>-0.24502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227370</td>\n",
       "      <td>0.11364</td>\n",
       "      <td>-0.237050</td>\n",
       "      <td>-0.31441</td>\n",
       "      <td>-0.432310</td>\n",
       "      <td>-0.10549</td>\n",
       "      <td>0.148660</td>\n",
       "      <td>-0.242560</td>\n",
       "      <td>0.47029</td>\n",
       "      <td>0.23420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>-0.49705</td>\n",
       "      <td>0.716420</td>\n",
       "      <td>0.40119</td>\n",
       "      <td>-0.05761</td>\n",
       "      <td>0.836140</td>\n",
       "      <td>0.825600</td>\n",
       "      <td>0.089630</td>\n",
       "      <td>-0.534920</td>\n",
       "      <td>0.34335</td>\n",
       "      <td>-0.27079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040066</td>\n",
       "      <td>0.60803</td>\n",
       "      <td>-0.027058</td>\n",
       "      <td>0.15273</td>\n",
       "      <td>-0.168870</td>\n",
       "      <td>-0.47664</td>\n",
       "      <td>-0.617750</td>\n",
       "      <td>-0.987350</td>\n",
       "      <td>0.23776</td>\n",
       "      <td>0.39952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matrix</th>\n",
       "      <td>-0.26638</td>\n",
       "      <td>0.444910</td>\n",
       "      <td>0.32743</td>\n",
       "      <td>0.43459</td>\n",
       "      <td>0.105280</td>\n",
       "      <td>0.317030</td>\n",
       "      <td>-0.345030</td>\n",
       "      <td>0.181470</td>\n",
       "      <td>-0.14878</td>\n",
       "      <td>0.84897</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.106600</td>\n",
       "      <td>0.35388</td>\n",
       "      <td>-0.263550</td>\n",
       "      <td>0.59609</td>\n",
       "      <td>1.133400</td>\n",
       "      <td>-1.10250</td>\n",
       "      <td>0.776820</td>\n",
       "      <td>-0.172670</td>\n",
       "      <td>-0.53726</td>\n",
       "      <td>0.15800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1         2        3        4         5         6         7  \\\n",
       "0                                                                               \n",
       "ultimately  0.45483 -0.102290  0.35265  0.16507  0.333910 -0.033643  0.000439   \n",
       "want       -0.17124  0.564470  0.34667 -0.56711 -0.656750  0.120810 -0.768630   \n",
       "turn       -0.08525  0.021085  0.30965 -0.17603  0.088869  0.285050 -0.344560   \n",
       "text       -0.49705  0.716420  0.40119 -0.05761  0.836140  0.825600  0.089630   \n",
       "matrix     -0.26638  0.444910  0.32743  0.43459  0.105280  0.317030 -0.345030   \n",
       "\n",
       "                   8        9       10  ...        91       92        93  \\\n",
       "0                                       ...                                \n",
       "ultimately -0.093160  0.60292 -0.26628  ... -0.352140 -0.47388 -0.192560   \n",
       "want        0.072832  0.42237 -0.10464  ... -0.013218 -0.20853  0.052186   \n",
       "turn        0.113960  0.29212 -0.24502  ... -0.227370  0.11364 -0.237050   \n",
       "text       -0.534920  0.34335 -0.27079  ...  0.040066  0.60803 -0.027058   \n",
       "matrix      0.181470 -0.14878  0.84897  ... -1.106600  0.35388 -0.263550   \n",
       "\n",
       "                 94        95       96        97        98       99      100  \n",
       "0                                                                             \n",
       "ultimately  0.27630 -0.053913  0.34036  0.237230  0.054277  0.10977  0.18678  \n",
       "want       -0.86911 -0.858160 -0.23443  0.057799  0.031150  0.48789  0.69311  \n",
       "turn       -0.31441 -0.432310 -0.10549  0.148660 -0.242560  0.47029  0.23420  \n",
       "text        0.15273 -0.168870 -0.47664 -0.617750 -0.987350  0.23776  0.39952  \n",
       "matrix      0.59609  1.133400 -1.10250  0.776820 -0.172670 -0.53726  0.15800  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = pd.read_csv('../data/glove.csv', index_col=0)\n",
    "glove.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using spacy to prepare text\n",
    "\n",
    "The [spacy](https://spacy.io/usage/vectors-similarity) package can do a lot of the preparation we have described for us before using machine learning models.\n",
    "It can also do some other interesting things using the respresentation it creates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "example_doc = nlp(example_text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ultimately, we want to turn our text into a matrix that gives the algorithm information to categorize text. That is more difficult if we miss the same words due to case, punctuation, or common words that don't help predict. So, we can clean our text to potentially make our predictions better."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ultimately,\n",
       " ,,\n",
       " we,\n",
       " want,\n",
       " to,\n",
       " turn,\n",
       " our,\n",
       " text,\n",
       " into,\n",
       " a,\n",
       " matrix,\n",
       " that,\n",
       " gives,\n",
       " the,\n",
       " algorithm,\n",
       " information,\n",
       " to,\n",
       " categorize,\n",
       " text,\n",
       " .,\n",
       " That,\n",
       " is,\n",
       " more,\n",
       " difficult,\n",
       " if,\n",
       " we,\n",
       " miss,\n",
       " the,\n",
       " same,\n",
       " words,\n",
       " due,\n",
       " to,\n",
       " case,\n",
       " ,,\n",
       " punctuation,\n",
       " ,,\n",
       " or,\n",
       " common,\n",
       " words,\n",
       " that,\n",
       " do,\n",
       " n't,\n",
       " help,\n",
       " predict,\n",
       " .,\n",
       " So,\n",
       " ,,\n",
       " we,\n",
       " can,\n",
       " clean,\n",
       " our,\n",
       " text,\n",
       " to,\n",
       " potentially,\n",
       " make,\n",
       " our,\n",
       " predictions,\n",
       " better,\n",
       " .]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at each item in the text.\n",
    "[i for i in example_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# We can see here that each element has a vector representation.\n",
    "# Also, each vector is in 300 dimensions.\n",
    "print(len([i.vector for i in example_doc]))\n",
    "print(len([i.vector for i in example_doc][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is a vector for the document, which is the average of the element vectors.\n",
    "len(example_doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04557627,  0.22923034, -0.24733946,  0.01531771,  0.06508073,\n",
       "        0.0169166 , -0.03391241,  0.00259539, -0.00466278,  2.2064903 ,\n",
       "       -0.17149216,  0.06919597,  0.17887476, -0.00224403, -0.18808614,\n",
       "       -0.05013261, -0.09594608,  1.2901341 , -0.2074607 , -0.04749772,\n",
       "       -0.01562062,  0.03571492, -0.01421114, -0.01695105,  0.0155637 ,\n",
       "        0.1137066 , -0.03556239, -0.02754606,  0.11727446, -0.17732875,\n",
       "       -0.02521809,  0.06882533, -0.02938912,  0.02645194,  0.10027047,\n",
       "       -0.05578014,  0.11198515,  0.01748868, -0.07824575, -0.10478983],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at just the first 40 elements of the vector.\n",
    "example_doc.vector[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8728022575670232"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can calculate similarity between documents, too.\n",
    "example_2 = nlp('Cleaning text is a good idea.')\n",
    "example_doc.similarity(example_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8728023]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we calculate the cosine similarity of the two vectors,\n",
    "# we can see that it is the method used above.\n",
    "cosine_similarity([example_doc.vector], [example_2.vector])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakout Exercises\n",
    "\n",
    "Let's do an exercise to reinforce the concepts we learned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EX1: similarity\n",
    "\n",
    "Let's use spacy to help us compute the similarity of two strings.\n",
    "\n",
    "1. Create two spacy documents, named `breakout1` and `breakout2`, by passing strings to the `nlp()` vector package that we created earlier.\n",
    "1. Use the similarity method on the first document to compare it to the second.\n",
    "1. Try changing up the strings (and feel free to Google longer passages to test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-1 code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-2 code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
